{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ac0CUz9x4F7f",
    "outputId": "9ff49d32-6cce-4dc0-ff0a-225693ff8fd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  \n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk  \n",
    "\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DuG7keQV4SAM",
    "outputId": "1ed63ded-3f1a-416d-c899-95436407efcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8r5_MTU4VNg"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/OnePlus6T.labelled.csv')\n",
    "X = df.reviews\n",
    "y = df.intent.astype('category')\n",
    "dict( enumerate(y.cat.categories) )\n",
    "labels = y.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLd4vk8t4hx7"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for sen in range(0, len(X)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # remove all digits\n",
    "    document = re.sub(r'\\d+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [lemmatizer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YaXjF-B4s_P"
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hU6BJCdQ4wFB"
   },
   "outputs": [],
   "source": [
    "macronum=sorted(set(df['intent']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['intent']=df['intent'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRild-8K44c-"
   },
   "outputs": [],
   "source": [
    "for i in range(X.shape[0]):\n",
    "    text = X[i]\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "\n",
    "for i in df['intent']:\n",
    "    labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLPrqmpG49AZ"
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 1\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CyOYQxpI4_AW",
    "outputId": "0f93c4c4-d626-4b38-986a-26945126cdfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if(k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS):\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UpHqWFOl5Br8",
    "outputId": "6fb34b72-f107-4602-a411-13b2a3b40d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of 1043 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('No. of %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sBNHIx-K5E-6",
    "outputId": "05aec275-d055-4102-e23c-fd16adb7a9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (320, 1, 100)\n",
      "Shape of label tensor: (320, 3)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_6nJ5d95Hy7"
   },
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bdeQwezh5LfL",
    "outputId": "27899121-e238-48e2-9250-fc865a4d2494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/content/drive/My Drive/glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xc1U1FCy5OKT"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlcT40nJc46_"
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "pkKdE-CU5PXq",
    "outputId": "bdd28161-29dc-490d-cd45-fe00b8761eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 400)            626200    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1, 400)            961600    \n",
      "_________________________________________________________________\n",
      "att_layer_6 (AttLayer)       (None, 400)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1203      \n",
      "=================================================================\n",
      "Total params: 1,629,203\n",
      "Trainable params: 1,629,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(200, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(LSTM(200, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1465
    },
    "colab_type": "code",
    "id": "LdO3Oms05UtN",
    "outputId": "e731ceb0-c18b-4b38-b07c-fd16f23650ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 256 samples, validate on 64 samples\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 56s 221ms/step - loss: 0.9001 - acc: 0.6914 - val_loss: 0.7469 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71875, saving model to model_attn.hdf5\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 52s 205ms/step - loss: 0.6680 - acc: 0.7383 - val_loss: 0.8573 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71875 to 0.75000, saving model to model_attn.hdf5\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.4722 - acc: 0.8320 - val_loss: 0.5145 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75000 to 0.79688, saving model to model_attn.hdf5\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 52s 205ms/step - loss: 0.3452 - acc: 0.8398 - val_loss: 0.5518 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79688 to 0.82812, saving model to model_attn.hdf5\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.2521 - acc: 0.9102 - val_loss: 0.4647 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82812\n",
      "Epoch 6/20\n",
      "256/256 [==============================] - 53s 205ms/step - loss: 0.1864 - acc: 0.9531 - val_loss: 0.4728 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82812\n",
      "Epoch 7/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1723 - acc: 0.9531 - val_loss: 0.5610 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.82812 to 0.84375, saving model to model_attn.hdf5\n",
      "Epoch 8/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.1116 - acc: 0.9766 - val_loss: 1.1861 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.84375\n",
      "Epoch 9/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.0691 - acc: 0.9805 - val_loss: 1.6109 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84375\n",
      "Epoch 10/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.0876 - acc: 0.9844 - val_loss: 1.6501 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84375\n",
      "Epoch 11/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.0436 - acc: 0.9883 - val_loss: 1.2389 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.84375\n",
      "Epoch 12/20\n",
      "256/256 [==============================] - 52s 203ms/step - loss: 0.0386 - acc: 0.9883 - val_loss: 2.0176 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.84375\n",
      "Epoch 13/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.0051 - acc: 0.9961 - val_loss: 1.3563 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84375\n",
      "Epoch 14/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 0.0043 - acc: 0.9961 - val_loss: 2.5822 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84375\n",
      "Epoch 15/20\n",
      "256/256 [==============================] - 53s 206ms/step - loss: 0.0479 - acc: 0.9961 - val_loss: 1.6872 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84375\n",
      "Epoch 16/20\n",
      "256/256 [==============================] - 52s 204ms/step - loss: 6.2551e-05 - acc: 1.0000 - val_loss: 1.8741 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84375\n",
      "Epoch 17/20\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 2.3423e-07 - acc: 1.0000 - val_loss: 2.2411 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84375\n",
      "Epoch 18/20\n",
      "256/256 [==============================] - 54s 212ms/step - loss: 1.2503e-07 - acc: 1.0000 - val_loss: 2.3166 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84375\n",
      "Epoch 19/20\n",
      "256/256 [==============================] - 54s 211ms/step - loss: 0.1133 - acc: 0.9883 - val_loss: 1.9568 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84375\n",
      "Epoch 20/20\n",
      "256/256 [==============================] - 53s 208ms/step - loss: 2.3390e-04 - acc: 1.0000 - val_loss: 1.7904 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84375\n"
     ]
    }
   ],
   "source": [
    "cp=ModelCheckpoint('model_attn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=2,callbacks=[cp])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HAN_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
